import os
import json
import logging
from typing import List, Dict, Tuple
from anthropic import AsyncAnthropic, AnthropicError
import numpy as np
import mlflow
from fastapi import HTTPException
from scipy import stats
from dataclasses import dataclass

from ..models.schemas import ClaudeJudgment

logger = logging.getLogger(__name__)


@dataclass
class UncertaintyMetrics:
    """Uncertainty quantification metrics for UQLM integration."""
    variance: float
    confidence_interval: Tuple[float, float]
    entropy: float
    abstention_threshold: float = 0.3


class ClaudeJudge:
    """
    Enhanced LLM-as-a-Judge implementation using Claude Sonnet 4.5.
    
    October 2025 Enhancements:
    - Increased self-consistency sampling (5 generations vs 3)
    - Weighted consensus voting with uncertainty calibration
    - UQLM-inspired uncertainty quantification for sub-1% false positives
    - Abstention logic for high-uncertainty cases (>0.3 variance)
    
    Target: 95%+ accuracy with 25-35% false positive reduction
    """
    
    def __init__(self, api_key: str):
        self.client = AsyncAnthropic(api_key=api_key)
        self.model = "claude-sonnet-4-5-20250929"
        self.num_samples = 5  # Enhanced: 5 samples for better consensus (up from 3)
        self.temperature = 0.1  # Low temperature for consistency
        self.max_tokens = 500
        
        # UQLM Integration - Uncertainty thresholds
        self.abstention_threshold = 0.3  # Abstain if variance > 0.3
        self.confidence_threshold = 0.7  # High confidence threshold
        self.uncertainty_penalty = 0.1  # Penalty for uncertain predictions
        
        logger.info(f"Initialized Enhanced ClaudeJudge with {self.num_samples} samples and UQLM integration")

    async def evaluate(
        self,
        agent_output: str,
        ground_truth: str,
        history: List[str]
    ) -> ClaudeJudgment:
        """
        Evaluate agent output for hallucinations using self-consistency method.
        
        Args:
            agent_output: The text generated by the AI agent being tested
            ground_truth: Reference truth for comparison
            history: Conversation history for multi-turn context
            
        Returns:
            ClaudeJudgment with consensus score, explanation, and flagged segments
        """
        prompt = self._build_prompt(agent_output, ground_truth, history)
        
        try:
            with mlflow.start_run(nested=True):
                mlflow.log_param("model", self.model)
                mlflow.log_param("prompt_length", len(prompt))
                mlflow.log_param("num_samples", self.num_samples)
                
                # Enhanced Self-consistency: Generate 5 independent evaluations
                samples = []
                for i in range(self.num_samples):
                    response = await self.client.messages.create(
                        model=self.model,
                        max_tokens=self.max_tokens,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.temperature,
                        # Enable prompt caching for 30-50% cost reduction
                        extra_headers={"anthropic-beta": "prompt-caching-2024-07-31"}
                    )
                    
                    # Parse JSON response
                    sample = self._parse_response(response.content[0].text)
                    samples.append(sample)
                    mlflow.log_metric(f"sample_{i}_score", sample["score"])
                    logger.debug(f"Sample {i} score: {sample['score']}")
                
                # Enhanced weighted consensus with uncertainty quantification
                result, uncertainty_metrics = self._build_weighted_consensus(samples)
                
                # Log enhanced metrics
                mlflow.log_metric("consensus_score", result.score)
                mlflow.log_metric("uncertainty_variance", uncertainty_metrics.variance)
                mlflow.log_metric("uncertainty_entropy", uncertainty_metrics.entropy)
                mlflow.log_param("abstention_triggered", uncertainty_metrics.variance > self.abstention_threshold)
                
                return result
                
        except AnthropicError as e:
            logger.error(f"Claude API error: {e}")
            raise HTTPException(
                status_code=503,
                detail=f"Claude API unavailable: {str(e)}"
            )

    async def evaluate_async(
        self,
        agent_output: str,
        ground_truth: str,
        conversation_history: List[str] = None
    ) -> Dict:
        """
        Async wrapper for compatibility with existing code.
        Returns dict format for backward compatibility.
        """
        if conversation_history is None:
            conversation_history = []
            
        judgment = await self.evaluate(agent_output, ground_truth, conversation_history)
        
        return {
            "score": judgment.score,
            "reasoning": judgment.explanation,
            "hallucinated_segments": judgment.hallucinated_segments,
            "samples": judgment.samples
        }

    def _build_prompt(self, agent_output: str, ground_truth: str, history: List[str]) -> str:
        """Build constitutional AI prompt for Claude evaluation."""
        history_str = json.dumps(history) if history else "[]"
        
        return f"""You are an impartial AI judge evaluating factual accuracy.

Agent Output: {agent_output}
Ground Truth/Reference: {ground_truth}
Conversation History: {history_str}

Evaluate:
1. Score factual accuracy 0-1 (1=fully accurate, 0=completely hallucinated)
2. Explain any inaccuracies or hallucinations  
3. List specific hallucinated segments

Respond with ONLY valid JSON in this exact format (no markdown, no additional text):
{{"score": 0.0, "explanation": "your explanation here", "hallucinated_segments": ["segment1", "segment2"]}}"""

    def _parse_response(self, response_text: str) -> dict:
        """Parse and validate JSON response from Claude."""
        try:
            # Extract JSON from response (handle markdown code blocks if present)
            text = response_text.strip()
            if text.startswith("```json"):
                text = text.split("```json")[1].split("```")[0].strip()
            elif text.startswith("```"):
                text = text.split("```")[1].split("```")[0].strip()
            
            # Try to find JSON object if embedded in text
            if not text.startswith("{"):
                start = text.find("{")
                end = text.rfind("}") + 1
                if start >= 0 and end > start:
                    text = text[start:end]
            
            parsed = json.loads(text)
            
            # Ensure required fields exist
            if "score" not in parsed:
                parsed["score"] = 0.5
            if "explanation" not in parsed:
                parsed["explanation"] = "Unable to parse explanation"
            if "hallucinated_segments" not in parsed:
                parsed["hallucinated_segments"] = []
            
            return parsed
            
        except Exception as e:
            logger.error(f"Failed to parse Claude response: {e}")
            logger.debug(f"Raw response: {response_text[:500]}")
            # Return safe default
            return {
                "score": 0.5,
                "explanation": f"Parse error: {str(e)}",
                "hallucinated_segments": []
            }

    def _build_weighted_consensus(self, samples: List[dict]) -> Tuple[ClaudeJudgment, UncertaintyMetrics]:
        """
        Build weighted consensus with uncertainty quantification.
        
        October 2025 Enhancement:
        - Weighted voting based on confidence and consistency
        - UQLM-inspired uncertainty metrics for abstention logic
        - Sub-1% false positive rate targeting
        
        Returns:
            Tuple of (ClaudeJudgment, UncertaintyMetrics)
        """
        scores = np.array([float(s.get("score", 0.5)) for s in samples])
        
        # Calculate uncertainty metrics
        variance = float(np.var(scores))
        mean_score = float(np.mean(scores))
        std_score = float(np.std(scores))
        
        # Confidence interval (95%)
        confidence_interval = stats.t.interval(
            0.95, len(scores)-1, 
            loc=mean_score, 
            scale=stats.sem(scores)
        )
        
        # Entropy calculation for uncertainty
        # Normalize scores to probabilities
        probs = np.abs(scores - 0.5) + 0.1  # Add small epsilon
        probs = probs / np.sum(probs)
        entropy = float(-np.sum(probs * np.log(probs + 1e-10)))
        
        uncertainty_metrics = UncertaintyMetrics(
            variance=variance,
            confidence_interval=confidence_interval,
            entropy=entropy,
            abstention_threshold=self.abstention_threshold
        )
        
        # Weighted consensus calculation
        if variance > self.abstention_threshold:
            # High uncertainty: Apply conservative scoring
            logger.warning(f"High uncertainty detected (variance={variance:.3f}), applying abstention logic")
            
            # Conservative approach: penalize uncertain predictions
            final_score = mean_score - self.uncertainty_penalty
            final_score = max(0.0, min(1.0, final_score))  # Clamp to [0,1]
            
            explanation_prefix = f"[UNCERTAIN - Variance: {variance:.3f}] "
        else:
            # Low uncertainty: Use weighted average
            # Weight samples by inverse of their deviation from median
            median_score = np.median(scores)
            deviations = np.abs(scores - median_score)
            weights = 1.0 / (deviations + 0.1)  # Add epsilon to avoid division by zero
            weights = weights / np.sum(weights)  # Normalize
            
            final_score = float(np.average(scores, weights=weights))
            explanation_prefix = ""
        
        # Collect explanations (weighted by sample confidence)
        explanations = [s.get("explanation", "") for s in samples]
        # Select explanation from most confident sample (closest to consensus)
        best_idx = np.argmin(np.abs(scores - final_score))
        final_explanation = explanation_prefix + explanations[best_idx]
        
        # Aggregate hallucinated segments with frequency weighting
        segment_counts = {}
        for i, s in enumerate(samples):
            segs = s.get("hallucinated_segments", [])
            if isinstance(segs, list):
                for seg in segs:
                    if seg not in segment_counts:
                        segment_counts[seg] = 0
                    segment_counts[seg] += 1
        
        # Include segments mentioned by majority of samples
        threshold = len(samples) // 2
        final_segments = [seg for seg, count in segment_counts.items() if count > threshold]
        
        logger.info(f"Weighted consensus: {final_score:.3f} (variance={variance:.3f}, samples={len(samples)})")
        
        judgment = ClaudeJudgment(
            score=final_score,
            explanation=final_explanation,
            hallucinated_segments=final_segments,
            samples=samples
        )
        
        return judgment, uncertainty_metrics

    def _build_consensus(self, samples: List[dict]) -> ClaudeJudgment:
        """Legacy method - use _build_weighted_consensus instead."""
        judgment, _ = self._build_weighted_consensus(samples)
        return judgment

